from __future__ import division

from sklearn.cross_validation import KFold
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV
#from sklearn.metrics import classification_report
import sklearn.metrics as mtrx
from sklearn.neighbors import KNeighborsClassifier as KNC
from sklearn.ensemble import RandomForestClassifier as RF

import swap
import machine_utils as ml
#import metrics as mtrx
from metrics import compute_binary_metrics

from optparse import OptionParser
from astropy.table import Table
import pdb
import numpy as np
import datetime as dt 
import os, subprocess, sys
import cPickle


'''
Workflow:
   access morphology database
   accept labels/data for training
   accept labels/data for testing
   "whiten" data (normalize)
   {reduce dimensions} (optional)
   train the machine classifier
   run machine classifier on test sample
'''

def MachineClassifier(options, args):
    """
    NAME
        MachineClassifier.py

    PURPOSE
        Machine learning component of Galaxy Zoo Express

        Read in a training sample generated by human users (which have 
        preferentially been analyzed by SWAP).
        Learn on the training sample and moniter progress. 
        Once "fully trained", apply learned model to test sample. 

    COMMENTS
        Lots I'm sure. 

    FLAGS
        -h            Print this message
        -c            config file name 
    """

    # Check for setup file in array args:
    if (len(args) >= 1) or (options.configfile):
        if args: config = args[0]
        elif options.configfile: config = options.configfile
        print swap.doubledashedline
        print swap.ML_hello
        print swap.doubledashedline
        print "ML: taking instructions from",config
    else:
        print MachineClassifier.__doc__
        return

    tonights = swap.Configuration(config)
    
    # Read the pickled random state file
    random_file = open(tonights.parameters['random_file'],"r");
    random_state = cPickle.load(random_file);
    random_file.close();
    np.random.set_state(random_state);

    # Get the machine threshold (make retirement decisions)
    threshold = tonights.parameters['machine_threshold']
    prior = tonights.parameters['prior']

    # Get list of evaluation metrics and criteria   
    eval_metrics = tonights.parameters['evaluation_metrics']
    
    # How much cross-validation should we do? 
    cv = tonights.parameters['cross_validation']

    survey = tonights.parameters['survey']

    #----------------------------------------------------------------------
    # read in the metadata for all subjects (Test or Training sample?)
    storage = swap.read_pickle(tonights.parameters['metadatafile'], 'metadata')
    subjects = storage.subjects

    #----------------------------------------------------------------------
    # read in the SWAP collection
    sample = swap.read_pickle(tonights.parameters['samplefile'],'collection')

    #----------------------------------------------------------------------
    # read in or create the ML collection
    MLsample = swap.read_pickle(tonights.parameters['MLsamplefile'],
                                'MLcollection')

    # read in or create the ML bureau for machine agents (history)
    MLbureau = swap.read_pickle(tonights.parameters['MLbureaufile'],'bureau')
    #if not tonights.parameters['MLbureaufile']:
    #    MLbureaufile = swap.get_new_filename(tonights.parameters,'bureau','ML')
    #    tonights.parameters['MLbureaufile'] = MLbureaufile

    #MLbureau = swap.read_pickle(tonights.parameters['MLbureaufile'],'bureau')


    #-----------------------------------------------------------------------    
    #                 SELECT TRAINING & VALIDATION SAMPLES  
    #-----------------------------------------------------------------------
    # TO DO: training sample should only select those which are NOT part of 
    # validation sample (Nair catalog objects) 2/22/16

    train_sample = storage.fetch_subsample(sample_type='train',
                                           class_label='GZ2_label')
    
    try:
        train_meta, train_features = ml.extract_features(train_sample)
    except TypeError:
        print "ML: can't extract features from subsample."
        print "ML: Exiting MachineClassifier.py"
        sys.exit()
    else:
        # TODO: consider making this part of SWAP's duties? 
        train_labels = np.array([1 if p > prior else 0 for p in 
                                 train_meta['SWAP_prob']])
        #train_labels = train_meta['Nair_label'].filled()
        print "ML: found a training sample of %i subjects"%len(train_meta)
    

    valid_sample = storage.fetch_subsample(sample_type='valid',
                                           class_label='Expert_label')
    try:
        valid_meta, valid_features = ml.extract_features(valid_sample)
    except:
        print "ML: there are no subjects with the label 'valid'!"
    else:
        valid_labels = valid_meta['Expert_label'].filled()
        print "ML: found a validation sample of %i subjects"%len(valid_meta)

    # ---------------------------------------------------------------------
    # Require a minimum size training sample [Be reasonable, my good man!]
    # ---------------------------------------------------------------------
    if len(train_sample) < 500: 
        print "ML: training sample is too small to be worth anything."
        print "ML: Exiting MachineClassifier.py"
        sys.exit()
        
    else:
        print "ML: training sample is large enough to give it a shot."

        # TODO: LOOP THROUGH DIFFERENT MACHINES? 
        # 5/12/16 -- no... need to make THIS a class and create multiple 
        #            instances? Each one can be passed an instance of a machine?

        # Machine can be trained to maximize/minimize different metrics
        # (ACC, completeness, purity, etc. Have a list of acceptable ones.)
        # Minimize a Loss function (KNC doesn't have a loss fcn). 
        for metric in eval_metrics:
        
            # REGISTER Machine Classifier
            # Construct machine name --> Machine+Metric? For now: KNC
            machine = 'KNC'
            machine = 'RF'
            Name = machine+'_'+metric
        
            # register an Agent for this Machine
            # This "Agent" doesn't behave like a SW agent... at least not yet

            try: 
                test = MLbureau.member[Name]
            except: 
                MLbureau.member[Name] = swap.Agent_ML(Name, metric)
                
            MLagent = MLbureau.member[Name]

            #---------------------------------------------------------------    
            #     TRAIN THE MACHINE; EVALUATE ON VALIDATION SAMPLE
            #---------------------------------------------------------------

            # Now we run the machine -- need cross validation on whatever size 
            # training sample we have .. 
        
            # Fixed until we build in other machine options
            # Need to dynamically determine appropriate parameters...

            #max_neighbors = get_max_neighbors(train_features, cv)
            #n_neighbors = np.arange(1, (cv-1)*max_neighbors/cv, 5, dtype=int)
            #params = {'n_neighbors':n_neighbors, 
            #          'weights':('uniform','distance')}

            num_features = train_features.shape[1]
        
            min_features = int(round(np.sqrt(num_features)))
            params = {'max_features':np.arange(min_features, num_features+1),
                      'max_depth':np.arange(2,16)}

            # Create the model 
            # for "estimator=XXX" all you need is an instance of a machine -- 
            # any scikit-learn machine will do. However, non-sklearn machines..
            # That will be a bit trickier! (i.e. Phil's conv-nets)
            general_model = GridSearchCV(estimator=RF(n_estimators=30), 
                                         param_grid=params,
                                         error_score=0, scoring=metric, cv=cv) 
            
            # Train the model -- k-fold cross validation is embedded
            print "ML: Searching the hyperparameter space for values that "\
                "optimize the %s."%metric
            trained_model = general_model.fit(train_features, train_labels)

            MLagent.model = trained_model

            # Test "accuracy" (metric of choice) on validation sample
            score = trained_model.score(valid_features, valid_labels)

            time = dt.datetime.today().strftime('%Y-%m-%d')
            ratio = np.sum(train_labels==1) / len(train_labels)

            MLagent.record_training(model_described_by=
                                    trained_model.best_estimator_, 
                                    with_params=trained_model.best_params_, 
                                    trained_on=len(train_features), 
                                    with_ratio=ratio,
                                    at_time=time, 
                                    with_train_acc=trained_model.best_score_,
                                    and_valid_acc=trained_model.score(
                                        valid_features, valid_labels))

            # Compute / store confusion matrix as a function of threshold
            # produced by this machine on the Expert Validation sample
            #fps, tps, thresh = mtrx._binary_clf_curve(valid_labels,
            #                trained_model.predict_proba(valid_features)[:,1])
            fps, tps, thresh = mtrx.roc_curve(valid_labels, 
                            trained_model.predict_proba(valid_features)[:,1])
            #roc_score = mtrx.roc_auc_score(fps,tps)

            metric_list = compute_binary_metrics(fps, tps)
            ACC, TPR, FPR, FNR, TNR, PPV, FDR, FOR, NPV = metric_list
        
            MLagent.record_evaluation(accuracy=ACC, recall=TPR, precision=PPV,
                                      false_pos=FPR, completeness_f=TNR,
                                      contamination_f=NPV)
            
            #MLagent.plot_ROC()

            # ---------------------------------------------------------------
            # IF TRAINED MACHINE PREDICTS WELL ON VALIDATION ....
            # ---------------------------------------------------------------
            if MLagent.is_trained(metric):
                print "ML: %s has successfully trained and will be applied "\
                    "to the test sample."

                # Retrieve the test sample 
                test_sample = storage.fetch_subsample(sample_type='test',
                                                      class_label='GZ2_label')
                try:
                    test_meta, test_features = ml.extract_features(test_sample)
                except:
                    print "ML: there are no subjects with the label 'test'!"
                    print "ML: which means there's nothing more to do!"
                else:
                    print "ML: found test sample of %i subjects"%len(test_meta)

                #-----------------------------------------------------------    
                #                 APPLY MACHINE TO TEST SAMPLE
                #----------------------------------------------------------- 
                predictions = MLagent.model.predict(test_features)
                probabilities = MLagent.model.predict_proba(test_features)
                print "ML: %s has finished predicting labels for the test "\
                    "sample."%Name
                print "ML: Generating performance report on the test sample:"
                test_labels = test_meta['GZ2_label'].filled()
                print mtrx.classification_report(test_labels, predictions)
                #pdb.set_trace()
                
                # ----------------------------------------------------------
                # Save the predictions and probabilities to a new pickle

                test_meta['predictions'] = predictions
                test_meta['probability_of_smooth'] = probabilities[:,1]
                
                filename=tonights.parameters['trunk']+'_'+Name+'.pkl'
                swap.write_pickle(test_meta, filename)

                """
                for thing, pred, p in zip(test_meta, predictions,
                                          probabitilies):
                    
                    # IF MACHINE P >= THRESHOLD, INSERT INTO SWAP COLLECTION
                    # --------------------------------------------------------
                    if (p >= threshold) or (1-p >= threshold):
                        print "BOOM! WE'VE GOT A MACHINE-CLASSIFIED SUBJECT:"
                        print "Probability:", p
                        # Initialize the subject in SWAP Collection
                        ID = thing['asset_id']
                        sample.member[ID] = swap.Subject(ID, str(s['SDSS_id']), 
                                            location=s['external_ref']) 
                    sample.member[ID].retiredby = 'machine'
                
                    # Flag subject as 'INACTIVE' / 'DETECTED' / 'REJECTED'
                    # ----------------------------------------------------------
                    if p >= threshold:
                        sample.member[str(s['id'])].state = 'inactive'
                    elif 1-p >= threshold:
                        sample.member[str(s['id'])].status = 'rejected' 

                #"""
    
    
    # If is hasn't been done already, save the current directory
    # ---------------------------------------------------------------------
    tonights.parameters['dir'] = os.getcwd()+'/'+tonights.parameters['trunk']
    
    if not os.path.exists(tonights.parameters['dir']):
        os.makedirs(tonights.parameters['dir'])


    # Repickle all the shits
    # -----------------------------------------------------------------------
    if tonights.parameters['repickle']:

        new_samplefile = swap.get_new_filename(tonights.parameters,'collection')
        print "ML: saving SWAP subjects to "+new_samplefile
        swap.write_pickle(sample, new_samplefile)
        tonights.parameters['samplefile'] = new_samplefile
        
        new_samplefile=swap.get_new_filename(tonights.parameters,'MLcollection')
        print "ML: saving test sample subjects to "+new_samplefile
        swap.write_pickle(MLsample,new_samplefile)
        tonights.parameters['MLsamplefile'] = new_samplefile

        new_bureaufile=swap.get_new_filename(tonights.parameters,'bureau','ML')
        print "ML: saving MLbureau to "+new_bureaufile
        swap.write_pickle(MLbureau, new_bureaufile)
        tonights.parameters['MLbureaufile'] = new_bureaufile

        metadatafile = swap.get_new_filename(tonights.parameters,'metadata')
        print "ML: saving metadata to "+metadatafile
        swap.write_pickle(storage, metadatafile)
        tonights.parameters['metadatafile'] = metadatafile



    # Update configfile to reflect Machine additions
    # -----------------------------------------------------------------------
    configfile = 'update.config'

    random_file = open(tonights.parameters['random_file'],"w");
    random_state = np.random.get_state();
    cPickle.dump(random_state,random_file);
    random_file.close();
    swap.write_config(configfile, tonights.parameters)
    

    return



def get_max_neighbors(sample, cv_folds):
    # when performing cross validation using a KNN classifier, the number of 
    # nearest neighbors MUST be less than the sample size. 
    # Depending on how many folds one wishes their CV to compute, this changes
    # So! For the required number of folds, calculate the number of nearest 
    # neighbors which would be ONE less than the length of the sample size
    # once the FULL size of the sample has been split into num_folds groups
    # for cross validation. 
    # Furthermore, if we have a massively huge sample, we don't actually want 
    # to search the ENTIRE n_neighbors parameter space. Increasing the 
    # neighbors effectively smooths over the noise and we don't want to smooth 
    # TOO much. SO, return a capped value --
    # Minimum sample size = 100 right now, so max neighbors == 99

    cv_size = len(sample)*(1-1/cv_folds)-1
    max_neighbors = int(np.min([cv_size, 99]))
    return max_neighbors


if __name__ == '__main__':
    parser = OptionParser()

    parser.add_option("-c", dest="configfile", help="Name of config file")
    parser.add_option("-o", dest="offline", default=False, action='store_true',
                      help="Run in offline mode; e.g. on existing SWAP output.")
    parser.add_option("-v", "--verbose", action='store_true', dest="verbose")
    parser.add_option("-q", "--quiet", action="store_false", dest="verbose")

    (options, args) = parser.parse_args()

    MachineClassifier(options, args)

